{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-quarter",
   "metadata": {},
   "source": [
    "# ML-MOGA User Manual\n",
    "\n",
    "Languages: Python(3.7.9), C++ <br />\n",
    "Tools: PyTorch(1.4.0+), Jupyter Notebook(6.2.0) <br />\n",
    "CAM(cam.lbl.gov): Data processing and training <br />\n",
    "NERSC: MOGA running(both Tr-MOGA & ML-MOGA)\n",
    "\n",
    "If you have any question, contact yupinglu89@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formed-monte",
   "metadata": {},
   "source": [
    "## Tr-MOGA \n",
    "\n",
    "#### 1. Load modules on NERSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "green-alloy",
   "metadata": {},
   "outputs": [],
   "source": [
    "module purge\n",
    "module load PrgEnv-gnu\n",
    "module load openmpi\n",
    "module load gsl\n",
    "module load pytorch/v1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "moved-mileage",
   "metadata": {},
   "source": [
    "#### 2. Set up MOGA code\n",
    "\n",
    "Example code(nl-run/run_ori/1_moga). <br />\n",
    "Compile the code in ML_package folder and get the executable by typing \"make\".<br />\n",
    "Submit scanjob.s to schedule the job on NERSC. The time limit on NERSC is 48 hrs for regular queue. You can also submit to 30 minutes debug queue for fast execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --qos=regular\n",
    "#SBATCH --time=48:00:00\n",
    "#SBATCH --nodes=64\n",
    "#SBATCH --tasks-per-node=32\n",
    "#SBATCH --constraint=haswell\n",
    "#SBATCH --mail-user=yupinglu89@gmail.com\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --job-name=MOGA\n",
    "\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "echo Working directory is : $SLURM_SUBMIT_DIR\n",
    "\n",
    "echo $SLURM_JOB_NODELIST\n",
    "echo $SLURM_JOBID\n",
    "echo $SLURM_NPROCS\n",
    "\n",
    "# NCPUS=`wc -l $SLURM_NODELIST| awk '{print $1}'`\n",
    "# JobID=`echo ${SLURM_JOBID} | cut -f1 -d.`\n",
    "\n",
    "NCPUS=$SLURM_NPROCS\n",
    "JobID=$SLURM_JOBID\n",
    "\n",
    "mkdir Dir_$JobID\n",
    "cd Dir_$JobID\n",
    "cp ../problem.cpp ./\n",
    "cp ../tracking.in ./\n",
    "cp ../scanjob.s ./\n",
    "cp ../ALSU.cpp ./\n",
    "cp ../ALSU.h ./\n",
    "cp ../input_gen.dat ./\n",
    "\n",
    "echo \"Start parallel job with CPUS\"\n",
    "echo $NCPUS\n",
    "echo \" -----------------------------------------------\"\n",
    "\n",
    "#module purge\n",
    "#module load PrgEnv-gnu\n",
    "#module load openmpi\n",
    "#module load gsl\n",
    "#module load pytorch\n",
    "####### Problem mode (0-DASearch, 1-FreqMap, 2-DiffMomen, 3-AreaMomen) ###########\n",
    "#####  1 for read pop, 2 for read gen\n",
    "EXEC=\"../nsga2r 0.5 3  2\"\n",
    "\n",
    "mpirun -v -np $NCPUS $EXEC <../tracking.in >$SLURM_SUBMIT_DIR/stdout_$JobID.out 2>$SLURM_SUBMIT_DIR/stderr_$JobID.out\n",
    "\n",
    "mv ../*$JobID.out ../slurm-*.out ../Dir_$JobID/\n",
    "\n",
    "### END of job\n",
    "echo \"Job complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-roommate",
   "metadata": {},
   "source": [
    "#### 3. Retrieve results (gen_*_db.dat files)\n",
    "\n",
    "These files are stored in Dir_* folder.<br />\n",
    "Delete the first line of each gen_*_db.dat file.\n",
    "\n",
    "     20 outputs,   11 variables,   conViol  rank  crowDist\n",
    "\n",
    "#### 4. Change random seeds\n",
    "\n",
    "We tested the moga code by changing two random seeds.<br />\n",
    "Change MOGA random seed: EXEC=\"../nsga2r 0.1 3  2\" (in scanjob.s)<br />\n",
    "Change lattice error random seed: srand(2021); (in problem.cpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-madonna",
   "metadata": {},
   "source": [
    "## Machine Learning Approach\n",
    "\n",
    "+ We first preprocess training data acquired from prior simulations and use this data to obtain two well-trained models using the neural network (NN) depicted below. \n",
    "+ We then use these two NN models to replace DA/MA particle tracking in MOGA while the rest of the MOGA setup remains the same as in the original tracking-based MOGA (Tr-MOGA). \n",
    "+ We evaluate the results.\n",
    "\n",
    "<img src=\"1.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "8-layer fully-connected (FC) NN architecture for DA and MA prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-sense",
   "metadata": {},
   "source": [
    "## ML-MOGA \n",
    "\n",
    "\n",
    "\n",
    "#### 1. Training Data\n",
    "\n",
    "We used the first 10 dat files as training data. These data are stored in dat folder on CAM. Below are two python scripts to preprocess the data (include filtering out those not meet the constraints)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-payment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "'''\n",
    "MOGA data preprocessing for dynamic aperture\n",
    "pre.da.py\n",
    "'''\n",
    "\n",
    "# load libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# load files\n",
    "path = './dat/'\n",
    "fs = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "# read files\n",
    "for f in fs:\n",
    "    tmp_df = pd.read_csv(path+f, header=None)\n",
    "    data.append(tmp_df)\n",
    "df = pd.concat(data, ignore_index=True, sort =False)\n",
    "\n",
    "# get X and Y\n",
    "data = df.to_numpy()\n",
    "x = data[:, -3]\n",
    "data = data[x == 0]\n",
    "X = data[:,20:31]\n",
    "Y_t = data[:,15:17]\n",
    "Y = np.mean(Y_t, axis=1)\n",
    "\n",
    "# split data into training set and test set\n",
    "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(X, Y, test_size=0.20, random_state=2020)\n",
    "\n",
    "# data normalization to [0, 1]\n",
    "x_mean = np.mean(x_train_o, axis=0)\n",
    "x_std = np.std(x_train_o, axis=0)\n",
    "print(x_mean)\n",
    "print(x_std)\n",
    "\n",
    "x_train = (x_train_o - x_mean) / x_std\n",
    "x_test = (x_test_o - x_mean) / x_std\n",
    "y_train = y_train_o\n",
    "y_test = y_test_o \n",
    "print(len(y_test))\n",
    "\n",
    "moga = {\n",
    "    \"x_train\": x_train,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\": y_test,\n",
    "    \"x_mean\": x_mean,\n",
    "    \"x_std\": x_std,\n",
    "}\n",
    "\n",
    "# save data\n",
    "pickle.dump(moga, open(\"da.1208.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "'''\n",
    "MOGA data preprocessing for momentum aperture\n",
    "pre.ma.py\n",
    "'''\n",
    "\n",
    "# load libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# load files\n",
    "path = './dat/'\n",
    "fs = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "# read files\n",
    "for f in fs:\n",
    "    tmp_df = pd.read_csv(path+f, header=None)\n",
    "    data.append(tmp_df)\n",
    "df = pd.concat(data, ignore_index=True, sort =False)\n",
    "\n",
    "# get X and Y\n",
    "data = df.to_numpy()\n",
    "x = data[:, -3]\n",
    "data = data[x == 0]\n",
    "X = data[:,20:31]\n",
    "Y = data[:,17]\n",
    "\n",
    "# split data into training set and test set\n",
    "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(X, Y, test_size=0.20, random_state=2020)\n",
    "\n",
    "# data normalization to [0, 1]\n",
    "x_mean = np.mean(x_train_o, axis=0)\n",
    "x_std = np.std(x_train_o, axis=0)\n",
    "print(x_mean)\n",
    "print(x_std)\n",
    "\n",
    "x_train = (x_train_o - x_mean) / x_std\n",
    "x_test = (x_test_o - x_mean) / x_std\n",
    "y_train = y_train_o\n",
    "y_test = y_test_o \n",
    "print(len(y_test))\n",
    "\n",
    "moga = {\n",
    "    \"x_train\": x_train,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\": y_test,\n",
    "    \"x_mean\": x_mean,\n",
    "    \"x_std\": x_std,\n",
    "}\n",
    "\n",
    "# save data\n",
    "pickle.dump(moga, open(\"ma.1208.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-livestock",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "known-laugh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "short-intermediate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operating-subscriber",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
