{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "honest-judge",
   "metadata": {},
   "source": [
    "# ML-MOGA User Manual\n",
    "\n",
    "Languages: Python(3.7.9), C++ <br />\n",
    "Tools: PyTorch(1.4.0+), Jupyter Notebook(6.2.0) <br />\n",
    "CAM(cam.lbl.gov): Data processing and training <br />\n",
    "NERSC: MOGA running(both Tr-MOGA & ML-MOGA)\n",
    "\n",
    "If you have any question, contact yupinglu89@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporate-knitting",
   "metadata": {},
   "source": [
    "## Tr-MOGA \n",
    "\n",
    "#### 1. Load modules on NERSC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weird-cosmetic",
   "metadata": {},
   "outputs": [],
   "source": [
    "module purge\n",
    "module load PrgEnv-gnu\n",
    "module load openmpi\n",
    "module load gsl\n",
    "module load pytorch/v1.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-prison",
   "metadata": {},
   "source": [
    "#### 2. Set up MOGA code\n",
    "\n",
    "Example code(nl-run/run_ori/1_moga). <br />\n",
    "Compile the code in ML_package folder and get the executable by typing \"make\".<br />\n",
    "Submit scanjob.s to schedule the job on NERSC. The time limit on NERSC is 48 hrs for regular queue. You can also submit to 30 minutes debug queue for fast execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-chassis",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/bin/bash\n",
    "#SBATCH --qos=regular\n",
    "#SBATCH --time=48:00:00\n",
    "#SBATCH --nodes=64\n",
    "#SBATCH --tasks-per-node=32\n",
    "#SBATCH --constraint=haswell\n",
    "#SBATCH --mail-user=yupinglu89@gmail.com\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --job-name=MOGA\n",
    "\n",
    "cd $SLURM_SUBMIT_DIR\n",
    "echo Working directory is : $SLURM_SUBMIT_DIR\n",
    "\n",
    "echo $SLURM_JOB_NODELIST\n",
    "echo $SLURM_JOBID\n",
    "echo $SLURM_NPROCS\n",
    "\n",
    "# NCPUS=`wc -l $SLURM_NODELIST| awk '{print $1}'`\n",
    "# JobID=`echo ${SLURM_JOBID} | cut -f1 -d.`\n",
    "\n",
    "NCPUS=$SLURM_NPROCS\n",
    "JobID=$SLURM_JOBID\n",
    "\n",
    "mkdir Dir_$JobID\n",
    "cd Dir_$JobID\n",
    "cp ../problem.cpp ./\n",
    "cp ../tracking.in ./\n",
    "cp ../scanjob.s ./\n",
    "cp ../ALSU.cpp ./\n",
    "cp ../ALSU.h ./\n",
    "cp ../input_gen.dat ./\n",
    "\n",
    "echo \"Start parallel job with CPUS\"\n",
    "echo $NCPUS\n",
    "echo \" -----------------------------------------------\"\n",
    "\n",
    "#module purge\n",
    "#module load PrgEnv-gnu\n",
    "#module load openmpi\n",
    "#module load gsl\n",
    "#module load pytorch\n",
    "####### Problem mode (0-DASearch, 1-FreqMap, 2-DiffMomen, 3-AreaMomen) ###########\n",
    "#####  1 for read pop, 2 for read gen\n",
    "EXEC=\"../nsga2r 0.5 3  2\"\n",
    "\n",
    "mpirun -v -np $NCPUS $EXEC <../tracking.in >$SLURM_SUBMIT_DIR/stdout_$JobID.out 2>$SLURM_SUBMIT_DIR/stderr_$JobID.out\n",
    "\n",
    "mv ../*$JobID.out ../slurm-*.out ../Dir_$JobID/\n",
    "\n",
    "### END of job\n",
    "echo \"Job complete\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "early-reliance",
   "metadata": {},
   "source": [
    "#### 3. Retrieve results (gen_*_db.dat files)\n",
    "\n",
    "These files are stored in Dir_* folder.<br />\n",
    "Delete the first line of each gen_*_db.dat file.\n",
    "\n",
    "     20 outputs,   11 variables,   conViol  rank  crowDist\n",
    "\n",
    "#### 4. Change random seeds\n",
    "\n",
    "We tested the moga code by changing two random seeds.<br />\n",
    "Change MOGA random seed: EXEC=\"../nsga2r 0.1 3  2\" (in scanjob.s)<br />\n",
    "Change lattice error random seed: srand(2021); (in problem.cpp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-glossary",
   "metadata": {},
   "source": [
    "## Machine Learning Approach\n",
    "\n",
    "+ We first preprocess training data acquired from prior simulations and use this data to obtain two well-trained models using the neural network (NN) depicted below. \n",
    "+ We then use these two NN models to replace DA/MA particle tracking in MOGA while the rest of the MOGA setup remains the same as in the original tracking-based MOGA (Tr-MOGA). \n",
    "+ We evaluate the results.\n",
    "\n",
    "<img src=\"1.png\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "8-layer fully-connected (FC) NN architecture for DA and MA prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "homeless-answer",
   "metadata": {},
   "source": [
    "## ML-MOGA \n",
    "\n",
    "\n",
    "\n",
    "#### 1. Training Data\n",
    "\n",
    "We used the first 10 dat files as training data. These data are stored in dat folder on CAM. Below are two python scripts to preprocess the data (include filtering out those not meet the constraints).\n",
    "\n",
    "python pre.da.py<br />\n",
    "python pre.ma.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-islam",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "'''\n",
    "MOGA data preprocessing for dynamic aperture\n",
    "pre.da.py\n",
    "'''\n",
    "\n",
    "# load libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# load files\n",
    "path = './dat/'\n",
    "fs = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "# read files\n",
    "for f in fs:\n",
    "    tmp_df = pd.read_csv(path+f, header=None)\n",
    "    data.append(tmp_df)\n",
    "df = pd.concat(data, ignore_index=True, sort =False)\n",
    "\n",
    "# get X and Y\n",
    "data = df.to_numpy()\n",
    "x = data[:, -3]\n",
    "data = data[x == 0]\n",
    "X = data[:,20:31]\n",
    "Y_t = data[:,15:17]\n",
    "Y = np.mean(Y_t, axis=1)\n",
    "\n",
    "# split data into training set and test set\n",
    "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(X, Y, test_size=0.20, random_state=2020)\n",
    "\n",
    "# data normalization to [0, 1]\n",
    "x_mean = np.mean(x_train_o, axis=0)\n",
    "x_std = np.std(x_train_o, axis=0)\n",
    "print(x_mean)\n",
    "print(x_std)\n",
    "\n",
    "x_train = (x_train_o - x_mean) / x_std\n",
    "x_test = (x_test_o - x_mean) / x_std\n",
    "y_train = y_train_o\n",
    "y_test = y_test_o \n",
    "print(len(y_test))\n",
    "\n",
    "moga = {\n",
    "    \"x_train\": x_train,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\": y_test,\n",
    "    \"x_mean\": x_mean,\n",
    "    \"x_std\": x_std,\n",
    "}\n",
    "\n",
    "# save data\n",
    "pickle.dump(moga, open(\"da.1208.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-edward",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "'''\n",
    "MOGA data preprocessing for momentum aperture\n",
    "pre.ma.py\n",
    "'''\n",
    "\n",
    "# load libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "\n",
    "# load files\n",
    "path = './dat/'\n",
    "fs = os.listdir(path)\n",
    "data = []\n",
    "\n",
    "# read files\n",
    "for f in fs:\n",
    "    tmp_df = pd.read_csv(path+f, header=None)\n",
    "    data.append(tmp_df)\n",
    "df = pd.concat(data, ignore_index=True, sort =False)\n",
    "\n",
    "# get X and Y\n",
    "data = df.to_numpy()\n",
    "x = data[:, -3]\n",
    "data = data[x == 0]\n",
    "X = data[:,20:31]\n",
    "Y = data[:,17]\n",
    "\n",
    "# split data into training set and test set\n",
    "x_train_o, x_test_o, y_train_o, y_test_o = train_test_split(X, Y, test_size=0.20, random_state=2020)\n",
    "\n",
    "# data normalization to [0, 1]\n",
    "x_mean = np.mean(x_train_o, axis=0)\n",
    "x_std = np.std(x_train_o, axis=0)\n",
    "print(x_mean)\n",
    "print(x_std)\n",
    "\n",
    "x_train = (x_train_o - x_mean) / x_std\n",
    "x_test = (x_test_o - x_mean) / x_std\n",
    "y_train = y_train_o\n",
    "y_test = y_test_o \n",
    "print(len(y_test))\n",
    "\n",
    "moga = {\n",
    "    \"x_train\": x_train,\n",
    "    \"x_test\": x_test,\n",
    "    \"y_train\": y_train,\n",
    "    \"y_test\": y_test,\n",
    "    \"x_mean\": x_mean,\n",
    "    \"x_std\": x_std,\n",
    "}\n",
    "\n",
    "# save data\n",
    "pickle.dump(moga, open(\"ma.1208.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pediatric-township",
   "metadata": {},
   "source": [
    "#### 2. Model Training\n",
    "\n",
    "The two model scripts will load ma.1208.pkl and da.1208.pkl and start the model training.\n",
    "\n",
    "nohup python ma.py > ma.out 2> ma.err < /dev/null & <br />\n",
    "nohup python da.py > da.out 2> da.err < /dev/null &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# ma.py\n",
    "# load libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# definition of a simple fc model [11, 32, 64, 1]\n",
    "class MOGANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MOGANet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(11, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class MOGADataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "# define train and test functions\n",
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # compute output\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss\n",
    "\n",
    "# Use CUDA\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(2020) \n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed_all(2020)\n",
    "\n",
    "# load files\n",
    "moga = pickle.load(open(\"ma.1208.pkl\", \"rb\"))\n",
    "x_train, x_test = moga[\"x_train\"], moga[\"x_test\"]\n",
    "y_train, y_test = moga[\"y_train\"], moga[\"y_test\"]\n",
    "x_mean, x_std = moga[\"x_mean\"], moga[\"x_std\"]\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train.reshape(-1,1)).float()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test.reshape(-1,1)).float()\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 128,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 4}\n",
    "\n",
    "# dataloader\n",
    "train_data = MOGADataset(x_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_data, **params)\n",
    "\n",
    "test_data = MOGADataset(x_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_data, **params)\n",
    "\n",
    "model = MOGANet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8)\n",
    "\n",
    "# Training here\n",
    "for t in range(500):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion) \n",
    "    test_loss = test(model, device, test_loader, criterion)\n",
    "    if t%10 == 0:\n",
    "        print(t, train_loss, test_loss)\n",
    "    \n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "# save trained model\n",
    "torch.save(model.state_dict(), 'ma.1208.pth')\n",
    "#model.load_state_dict(torch.load(\"ma.1208.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honey-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# da.py \n",
    "# load libs\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# definition of a simple fc model [11, 32, 64, 1]\n",
    "class MOGANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MOGANet, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(11, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class MOGADataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "# define train and test functions\n",
    "def train(model, device, train_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for x, y in train_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # compute output\n",
    "        outputs = model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    return train_loss\n",
    "\n",
    "def test(model, device, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # compute output\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, y)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "    test_loss /= len(test_loader)\n",
    "    return test_loss\n",
    "\n",
    "# Use CUDA\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(2020) \n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed_all(2020)\n",
    "\n",
    "# load files\n",
    "moga = pickle.load(open(\"da.1208.pkl\", \"rb\"))\n",
    "x_train, x_test = moga[\"x_train\"], moga[\"x_test\"]\n",
    "y_train, y_test = moga[\"y_train\"], moga[\"y_test\"]\n",
    "x_mean, x_std = moga[\"x_mean\"], moga[\"x_std\"]\n",
    "\n",
    "x_train = torch.from_numpy(x_train).float()\n",
    "y_train = torch.from_numpy(y_train.reshape(-1,1)).float()\n",
    "x_test = torch.from_numpy(x_test).float()\n",
    "y_test = torch.from_numpy(y_test.reshape(-1,1)).float()\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 128,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 4}\n",
    "\n",
    "# dataloader\n",
    "train_data = MOGADataset(x_train, y_train)\n",
    "train_loader = DataLoader(dataset=train_data, **params)\n",
    "\n",
    "test_data = MOGADataset(x_test, y_test)\n",
    "test_loader = DataLoader(dataset=test_data, **params)\n",
    "\n",
    "model = MOGANet().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.8)\n",
    "\n",
    "# Training here\n",
    "for t in range(500):\n",
    "    train_loss = train(model, device, train_loader, optimizer, criterion) \n",
    "    test_loss = test(model, device, test_loader, criterion)\n",
    "    if t%10 == 0:\n",
    "        print(t, train_loss, test_loss)\n",
    "    \n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "# save trained model\n",
    "torch.save(model.state_dict(), 'da.1208.pth')\n",
    "#model.load_state_dict(torch.load(\"da.1208.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "herbal-stationery",
   "metadata": {},
   "source": [
    "#### 3. Format converting\n",
    "\n",
    "We need to convert two generated model files ending with .pth to .pt format. The new files da.1208.pt and ma.1208.pt will be inserted into MOGA code to start ML-MOGA run. We first need to provide the current mean and std of our training data to ts.da.py and ts.ma.py before the conversion.\n",
    "\n",
    "python ts.da.py <br />\n",
    "python ts.ma.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understood-proposal",
   "metadata": {},
   "source": [
    "#### 4. ML-MOGA run\n",
    "\n",
    "1) Copy the converted model files (da.1208.pt and ma.1208.pt) to ML_package.\n",
    "\n",
    "2) Copy libtorch library files to ML_package.\n",
    "\n",
    "3) Replace problem.cpp with PyTorch models. The ML version of problem.cpp is listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominican-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "#include <stdio.h>\n",
    "#include <stdlib.h>\n",
    "#include <string.h>\n",
    "#include <math.h>\n",
    "#include <unistd.h>\n",
    "#include \"Tracy.Meso.h\"\n",
    "#include \"ALSU.h\"\n",
    "#include \"global.h\"\n",
    "#include <torch/script.h>\n",
    "#include <iostream>\n",
    "#include <stdexcept>\n",
    "#include <string>\n",
    "\n",
    "extern ALS_U_V17 *SR;\n",
    "\n",
    "string exec(string command) {\n",
    "   char buffer[128];\n",
    "   string result = \"\";\n",
    "\n",
    "   // Open pipe to file\n",
    "   FILE* pipe = popen(command.c_str(), \"r\");\n",
    "   if (!pipe) {\n",
    "      return \"popen failed!\";\n",
    "   }\n",
    "\n",
    "   // read till end of process:\n",
    "   while (!feof(pipe)) {\n",
    "\n",
    "      // use buffer to read and add to result\n",
    "      if (fgets(buffer, 128, pipe) != NULL)\n",
    "         result += buffer;\n",
    "   }\n",
    "\n",
    "   pclose(pipe);\n",
    "   return result;\n",
    "}\n",
    "\n",
    "double SegQ, SegB;\n",
    "ALS_U_V17 *Ring() {\n",
    "    SegQ = 1;\n",
    "    SegB = 3;\n",
    "    SR = new ALS_U_V17(SegQ,SegB);\n",
    "\n",
    "    SR->Individualize();\n",
    "    SR->SetIntegrator(4);\n",
    "    SR->SetSec6(10);\n",
    "\n",
    "    return SR;\n",
    "}\n",
    "\n",
    "void test_problem (int gen, int nload, double *xvar,  double *obj, double *constr,double *db) {\n",
    "    int i,j;\n",
    "    double maxBx=0,maxBy=0,maxEta=0;\n",
    "    double B3Betax = 0,B3Betay=0;\n",
    "    Belement *C;\n",
    "\n",
    "    SR->clearQuadGradErr();\n",
    "    SR->clearQuadSkew();\n",
    "    SR->ClearRef();\n",
    "    SR->ClearCOD();\n",
    "    SR->FixedPoint.clear();\n",
    "    SR->FixedPoint6.clear();\n",
    "    SR->ClearRef6();\n",
    "    SR->SetdP(0.0);\n",
    "    SR->Sextpoles_TurnOff();\n",
    "\n",
    "    double kqf1 =  xvar[0];\n",
    "    double kqf2 =  xvar[1];\n",
    "    double kqf3 =  xvar[2];\n",
    "    double kqf4 =  xvar[3];\n",
    "    double kqf5 =  xvar[4];\n",
    "    double kqf6 =  xvar[5];\n",
    "    double kqd1 =  xvar[6];\n",
    "    double kb1 =  xvar[7];\n",
    "    double kb2 =  xvar[8];\n",
    "    double kb3 =  xvar[8];\n",
    "    double kshh, kshh2; \n",
    "    // Input for training mdoel\n",
    "    kshh = xvar[9];\n",
    "    kshh2 = xvar[10];\n",
    "\n",
    "    ////////////////////////                                                                                                                                   \n",
    "    // linear lattice                                                                                                                                          \n",
    "    ////////////////////                                                                                                                                       \n",
    "\n",
    "    SR->setKQf1(kqf1);\n",
    "    SR->setKQf2(kqf2);\n",
    "    SR->setKQf3(kqf3);\n",
    "    SR->setKQf4(kqf4);\n",
    "    SR->setKQf5(kqf5);\n",
    "    SR->setKQf6(kqf6);\n",
    "    SR->setKQd1(kqd1);\n",
    "    SR->Quads[7]->SetK(kb1);\n",
    "    SR->Quads[8]->SetK(kb2);\n",
    "    SR->Quads[9]->SetK(kb3);\n",
    "\n",
    "#ifdef TESTRUN\n",
    "    SR->GetTwiss(1);\n",
    "#endif\n",
    "\n",
    "    if(!SR->GetTwiss(1))\n",
    "    {\n",
    "        for (i=0;i<ndb;i++)     db[i]=999.0;\n",
    "        for (i=0;i<ncon;i++)    constr[i]=-1.0;\n",
    "        for (i=0;i<nobj;i++)    obj[i] = 999.0+i*fabs(xvar[0]);\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    SR->CalcIntegral();\n",
    "    SR->CalcEmittance(2.0E9);\n",
    "    SR->SetA44();\n",
    "  \n",
    "#ifdef TESTRUN\n",
    "\n",
    "    SR->GetTwiss(1);\n",
    "    SR->CalcIntegral();\n",
    "    SR->ListSynch(stdout);\n",
    "    SR->listTwissTable(\"Twiss.txt\");\n",
    "\n",
    "    C=&(SR->Belem[0]);\n",
    "    FILE *fid = fopen(\"magnet.txt\",\"w\");\n",
    "    for(int n=0; n< SR->NoB;n++) {\n",
    "        C = &(SR->Belem[n]);\n",
    "        fprintf(fid,\" %d   %s  %d   %d  \\n\", n, C->Elem->Name.c_str(), C->Elem->GetSec6(), C->Elem->IntegMethod );\n",
    "\n",
    "    }\n",
    "    fclose(fid);\n",
    "#endif\n",
    "\n",
    "    C=&(SR->Belem[0]);\n",
    "\n",
    "    db[0] = C->TwissH.v[1]; //betax at s=0;                                                                                                                  \n",
    "    db[1] = C->TwissV.v[1]; //betay at s = 0;                                                                                                                \n",
    "    db[10] = C->Eta.v[0];   //etax at s= 0;                                                                                                                  \n",
    "\n",
    "    int inj_sn,qf1_sn,qf2_sn,qf3_sn,qf4_sn,qf5_sn,qf6_sn;\n",
    "\n",
    "    for(int n=0; n< SR->NoB;n++) {\n",
    "        C = &(SR->Belem[n]);\n",
    "        if (C->TwissH.v[1]>maxBx) maxBx = C->TwissH.v[1];\n",
    "        if (C->TwissV.v[1]>maxBy) maxBy = C->TwissV.v[1];\n",
    "        if (C->Eta.v[0]>maxEta) maxEta = C->Eta.v[0];\n",
    "\n",
    "        if (strncmp(\"B3M\",C->Elem->Name.c_str(),3)==0) {\n",
    "            B3Betax = C->TwissH.v[1];\n",
    "            B3Betay = C->TwissV.v[1];\n",
    "        }\n",
    "\n",
    "        if(strncmp(\"SECT1\",C->Elem->Name.c_str(),5)==0) inj_sn = 0;\n",
    "        if(strncmp(\"QF1(01)\",C->Elem->Name.c_str(),7)==0) qf1_sn = n;\n",
    "        if(strncmp(\"QF2(01)\",C->Elem->Name.c_str(),7)==0) qf2_sn = n;\n",
    "        if(strncmp(\"QF6(01)\",C->Elem->Name.c_str(),7)==0) qf6_sn = n;\n",
    "    }                                                                                                \n",
    "\n",
    "    db[2] = SR->NtlEmittance;  //emittance                                                                                                    \n",
    "    db[3] = SR->A44.m[0][0]+SR->A44.m[1][1]; //tracex                                                                                                          \n",
    "    db[4] = SR->A44.m[2][2]+SR->A44.m[3][3]; //tracey                                                                                                          \n",
    "    db[5] = SR->Jx;  //jx                                                                                                                       \n",
    "    db[6] = SR->Jz;  //jy                                                                                                                     \n",
    "    db[7] = SR->Je;  //j                                                                                                                                      \n",
    "    db[8] = SR->TuneH;   //tunex                                                                                                                                 \n",
    "    db[9] = SR->TuneV;   //tuney     \n",
    "    db[11] = SR->ChromH;\n",
    "    db[12] = SR->ChromV;\n",
    "    db[13] = B3Betax;\n",
    "    db[14] = B3Betay;\n",
    "\n",
    "    constr[0] = 2-fabs(db[3]);\n",
    "    constr[1] = 2-fabs(db[4]);\n",
    "    constr[2] = db[5];\n",
    "    constr[3] = db[6];\n",
    "    constr[4] = db[7];\n",
    "    constr[5] = 30-maxBx;  // maximum betax <25 m                                                                                                                \n",
    "    constr[6] = 30-maxBy; // maximum betay <25 m                                                                                                                 \n",
    "    constr[7] = 0.15-maxEta; // maximum etax <15 cm                                                                                                              \n",
    "    constr[8] = (155e-12-db[2]);  // emit <150 pm-rad \n",
    "\n",
    "    double ftunex = fabs(db[8]-(int) db[8]);\n",
    "    double ftuney = fabs(db[9]-(int) db[9]);\n",
    "\n",
    "    constr[9] = (ftunex-0.1)*(0.4-ftunex);\n",
    "    constr[10] = (ftuney-0.1)*(0.4-ftuney);\n",
    "    constr[11] = 1e-3-fabs(db[10]); //etax <1mm at s = 0;                                                                                                        \n",
    "    constr[12] = (db[0]-1)*(5-db[0]); //1<betax<5\n",
    "    constr[13] = (db[1]-1)*(5-db[1]); //1<betay<5\n",
    "    constr[14] = (4-B3Betax)*(4-B3Betay);\n",
    "    constr[15] = 0.01-fabs(ftunex-ftuney);\n",
    "\n",
    "    /* normialize the constraints*/\n",
    "    for (i=0;i<16;i++) {\n",
    "        if (constr[i]<0||isnan(constr[i])) {\n",
    "#ifdef TESTRUN\n",
    "            printf(\" ----- constr vio %d, %f\\n\",i,constr[i]);\n",
    "#endif \n",
    "            constr[i]=-1;\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    obj[0]=db[2];\n",
    "    constr[16] = 0.0;\n",
    "    constr[17] = 0.0;\n",
    "\n",
    "    //////////////////////////                                                                                                                          \n",
    "    // nonliear optimziation                                                                                                                                   \n",
    "    //////////////////////////   \n",
    "\n",
    "    SR->Sexts[2]->SetK(kshh);\n",
    "    SR->Sexts[3]->SetK(kshh2);\n",
    "\n",
    "    if (!SR->FitChrom(1.0,1.0)) {\n",
    "        db[15]=0.0;\n",
    "        db[16]=0.0;\n",
    "        db[17]= 0.0;\n",
    "\n",
    "        db[18]=0.0;\n",
    "        db[19]= 0.0;\n",
    "        constr[16] = -1.0;\n",
    "        constr[17] = -1.0;\n",
    "        obj[1]=9999.0;\n",
    "\n",
    "        return;\n",
    "    }\n",
    "\n",
    "    db[18] = SR->GetSF();\n",
    "    db[19] = SR->GetSD();\n",
    "\n",
    "    constr[16] = 900-fabs(db[18]);\n",
    "    constr[17] = 900-fabs(db[19]);  \n",
    "\n",
    "    srand(199);\n",
    "    SR->setQuadSkew(5e-4);\n",
    "    SR->setQuadGradErr(2e-4);\n",
    "  \n",
    "#ifdef TESTRUN\n",
    "    printf(\" ---SF: %32.28f, SD:%32.28f\\n\",SR->GetSF(),SR->GetSD());\n",
    "    SR->GetTwiss(1);\n",
    "    SR->CalcIntegral();\n",
    "    SR->ListSynch(stdout);\n",
    "    SR->listTwissTable(\"Twiss_1.txt\");\n",
    "    SR->Tracy2AT(\"lattice.m\");\n",
    "#endif\n",
    "  \n",
    "    //double MomAper1[2],MomAper2[2],MomAper3[2],MomAper4[2];\n",
    "\n",
    "#ifdef TESTRUN\n",
    "    \n",
    "#else\n",
    "    // Input\n",
    "    float aa = (xvar[0] - 14.028049) / 3.80621681e-01;\n",
    "    float bb = (xvar[1] - 9.7394512) / 5.89510194e-01;\n",
    "    float cc = (xvar[2] - 11.783254) / 9.37556664e-01;\n",
    "    float dd = (xvar[3] - 15.524115) / 2.50345962e-01;\n",
    "    float ee = (xvar[4] - 15.713844) / 2.14431638e-01;\n",
    "    float ff = (xvar[5] - 15.480341) / 3.52844788e-01;\n",
    "    float gg = (xvar[6] - -14.873348) / 3.24311876e-01;\n",
    "    float hh = (xvar[7] - -2.3504979) / 1.64542081e-01;\n",
    "    float ii = (xvar[8] - -7.1688257) / 2.05577563e-01;\n",
    "    float jj = (xvar[9] - -75.974958) / 2.79143321e+02;\n",
    "    float kk = (xvar[10] - -157.57141) / 3.02909397e+02;\n",
    "\n",
    "    auto X = torch::tensor({aa, bb, cc, dd, ee, ff, gg, hh, ii, jj, kk}, torch::requires_grad(false).dtype(torch::kFloat32)).view({1,11});\n",
    "    vector<torch::jit::IValue> inputs;\n",
    "    inputs.push_back(X);\n",
    "\n",
    "    // da\n",
    "    torch::jit::script::Module module_da = torch::jit::load(\"da.1208.pt\");\n",
    "\n",
    "    // Execute the model and turn its output into a tensor.\n",
    "    at::Tensor output_da = module_da.forward(inputs).toTensor();\n",
    "    db[15] = output_da.item<float>();\n",
    "    db[16] = output_da.item<float>();\n",
    "    \n",
    "    // ma\n",
    "    torch::jit::script::Module module_ma = torch::jit::load(\"ma.1208.pt\");\n",
    "    \n",
    "    // Execute the model and turn its output into a tensor.\n",
    "    at::Tensor output_ma = module_ma.forward(inputs).toTensor();\n",
    "    db[17] = output_ma.item<float>();\n",
    "#endif\n",
    "\n",
    "    obj[1]=(db[15]+db[16])/2;\n",
    "    obj[2]=db[17];\n",
    "\n",
    "    if (isnan(obj[1])) obj[1]=9999.0;\n",
    "    if (isnan(obj[2])) obj[2]=9999.0; \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-ceiling",
   "metadata": {},
   "source": [
    "4) Remember to replace float aa - kk with the current mean and std, same as format conversion.\n",
    "\n",
    "5) Other minor changes please refer to NERSC/ml-run/7_moga/ML_package.\n",
    "\n",
    "6) Create build directory and run the following command to compile executable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "employed-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "make build\n",
    "\n",
    "cd build\n",
    "\n",
    "cmake -DCMAKE_CXX_COMPILER=mpiCC -DCMAKE_C_COMPILER=mpicc -DCMAKE_PREFIX_PATH=./libtorch ..\n",
    "\n",
    "make\n",
    "\n",
    "mv nsga2r .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-intent",
   "metadata": {},
   "source": [
    "7) Submit the job script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbatch scanjob.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-seventh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
